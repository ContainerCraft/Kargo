history
git stage -A; git commit -m 'working deploy from codespaces to omni'; git push
git status
clear
cleafr
omnictl cluster delete optiplexprime
omnictl cluster delete --help
omnictl get clusters
omnictl c
omnictl cluster s
omnictl cluster ls
clear; ll
cat metal/dev/apply.sh
ls metal/dev
ls metal
ls
clear; pulumi up --skip-preview --refresh=true
k get sc
pulumi up --skip-preview --refresh=true
pulumi up --skip-preview --refresh=false
pulumi preview
omnictl cluster template status -f metal/optiplexprime/omni-cluster.yaml
omnictl cluster template sync -f metal/optiplexprime/omni-cluster.yaml
pulumi down --skip-preview --refresh=true
vim pulumi/stacks/Pulumi.mordor.yaml
pulumi config set --path hostpath_provisioner.default_storage_class true
pulumi config set --path cdi.enabled true
pulumi config set --path prometheus.enabled true
pulumi up --skip-preview --refresh
pulumi config set --path hostpath_provisioner.enabled true
pulumi config set --path hostpath_provisioner.default_path /var/mnt/hostpath-provisioner
pulumi config
pulumi config set --path cnao.enabled true
pulumi config set --path multus.enabled true
ls pulumi/
vim metal/optiplexprime/omni-cluster.yaml
pulumi up --skip-preview
pulumi config set --path kubevirt.enabled true
pulumi config set --path cert_manager.enabled true
pulumi up
pulumi config set --path kubernetes.context usrbinkat-optiplexprime
k config get-contexts
pulumi config set --path kubernetes.kubeconfig $KUBECONFIG
omnictl get machines
k9s -A
eval (pulumi env open mordor --format=shell)
pulumi stack select --create mordor
git pull
direnv allow
vim .envrc
git checkout mvp/usrbinkat/refactor
git stage -A; git commit -m 'wip'; git push
git pull origin mvp/usrbinkat/refactor
git branch -a
k get po -A
pulumi env edit mordor
echo $KUBECONFIG
kubectl get po -A
export BROWSER=echo
eval (pulumi env open mordor --output=shell)
pulumi login
kubectl oidc-login
/bin/python3 /home/vscode/.vscode-remote/extensions/ms-python.python-2024.10.0-linux-x64/python_files/printEnvVariablesToFile.py /home/vscode/.vscode-remote/extensions/ms-python.python-2024.10.0-linux-x64/python_files/deactivate/fish/envVars.txt
clear
pulumi stack rm kargo
pulumi down --refresh=true --skip-preview
pulumi up --skip-preview --refresh=true
pulumi config
pulumi preview --refresh
cd /workspaces/Kargo/
cat controlplane.yaml
vim controlplane.yaml
talosctl gen config abcd https://192.168.1.2:6443
talosctl gen config .
talosctl gen config
talosctl gen
talosctl --help
cd t
mkdir t
cd /tmp/
k get all -n hostpath-provisioner
k get pvc
k get events -Aw
kubectl apply -f hack/ubuntu-br0-persistent.yaml
k get vm
docker pull docker.io/containercraft/ubuntu:22.04
docker pull docker.io/containercraft/ubuntu:24.04
pulumi up --skip-preview --refresh=false
pulumi config set --path cdi.enabled true
pulumi config set --path hostpath_provisioner.default_storage_class true
pulumi config set --path hostpath_provisioner.enabled true
pulumi config set --path cnao.enabled true
pulumi config set --path multus.enabled true
pulumi config set --path kubevirt.enabled true
pulumi config set --path cert_manager.enabled true
pulumi preview
pulumi config set --path kubernetes.context usrbinkat-optiplexprime
kubectl config get-contexts
pulumi stack select --create kargo
history
pulumi config get kubeconfig
pulumi config get
pulumi down --refresh=yes --skip-preview
k config get-contexts
pulumi config set --path kubernetes.context usrbinkat-optiplex
vim pulumi/stacks/Pulumi.kargo.yaml
pulumi stack ls
kubectl get po -A
echo $KUBECONFIG
echo $OMNICONFIG
omnictl cluster template status -f metal/optiplexprime/omni-cluster.yaml
omnictl cluster template sync -f metal/optiplexprime/omni-cluster.yaml
omnictl get machines
omnictl delete 4c4c4544-004a-4d10-8052-c4c04f4e5932
omnictl
env | grep -i pulumi
eval (pulumi env open --format=shell kargo)
pulumi login
